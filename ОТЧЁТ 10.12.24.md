# АНАЛИЗ ДАННЫХ И ИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ (Анализ данных в разработке игр)

Отчет по лабораторной работе #5 выполнил(а):
- Лопатин Никита Сергеевич
- РИ230948
Отметка о выполнении заданий (заполняется студентом):

| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * |  |
| Задание 2 | * |  |
| Задание 3 | * |  |

знак "*" - задание выполнено; знак "#" - задание не выполнено;

Работу проверили:
- к.т.н., доцент Денисов Д.В.
- к.э.н., доцент Панов М.А.
- ст. преп., Фадеев В.О.

[![N|Solid](https://cldup.com/dTxpPi9lDf.thumb.png)](https://nodesource.com/products/nsolid)

[![Build Status](https://travis-ci.org/joemccann/dillinger.svg?branch=master)](https://travis-ci.org/joemccann/dillinger)


## Цель работы
Познакомиться с программными средствами для создания системы машинного обучения и ее интеграции в Unity.


## Задание 1
Найдите внутри C# скрипта “коэффициент корреляции ” и сделать выводы о том, как он влияет на обучение модели.

В скрипте можно выделить следующие области, где есть корреляция:

CollectObservations;
Target.localPosition;
this.transform.localPosition;
rBody.velocity.x и rBody.velocity.z;
Cкорость агента может быть зависима от направления цели.
SetReward;
distanceToTarget - напрямую влияет на вознаграждение(SetReward);
Target.localPosition;

Если наблюдения, добавляемые в VectorSensor сильно коррелированы , это может негативно повлиять на обучение. 
В данном случае взаимосвязь между наблюдениями логична и отражает физику задачи.

Случайная инициализация позиции цели помогает уменьшить избыточную корреляцию между эпизодами, улучшая обобщающую способность модели.

Механизм вознаграждения базируется на расстоянии до цели, что поощряет агента к действиям, направленным на минимизацию этого расстояния. Это создает логическую связь между действиями и результатами.



## Задание 2

Изменить параметры файла yaml-агента и определить какие параметры и как влияют на обучение модели. Привести описание не менее трех параметров.

batch_size
Указывает количество данных, которые используются для одного обновления весов модели.
Влияние:
Быстрее обновляет веса модели, так как обрабатывает меньше данных.
Может привести к меньшей стабильности.
Делает обучение более стабильным, так как использует больше данных для каждого обновления.
Требует больше оперативной памяти и может замедлить обучение.
В данном случае значение 1024 сбалансировано, что позволяет эффективно обучать агента.

learning_rate

Скорость, с которой модель обновляет свои параметры на основе градиента.
Влияние:

Позволяет более точно сходиться к оптимуму, но может сделать обучение слишком медленным.
Ускоряет обучение, но может привести к нестабильности и "перескакиванию" оптимальных решений.
Линейное изменение (learning_rate_schedule: linear) дополнительно помогает уменьшить скорость обучения по мере прогресса, что стабилизирует обучение на поздних этапах.

gamma

Дисконтирующий фактор, который определяет важность будущих вознаграждений относительно текущих.
Влияние:
Модель больше фокусируется на текущем вознаграждении и менее учитывает будущие.
Значение 0.99 — стандартное для большинства задач. Оно позволяет агенту учитывать долгосрочные результаты, сохраняя при этом стабильность вычислений.


## Задание 3
Приведите примеры, для каких игровых задачи и ситуаций могут использоваться примеры 1 и 2 с ML-Agent’ом. В каких случаях проще использовать ML-агент, а не писать программную реализацию решения?

Пример 1: Задачи для обучения поведения персонажа в игре (типа RollerAgent)

Сложно заранее предугадать все возможные траектории движения или прописать логику, учитывающую поведение препятствий.
ML-агент может обучиться динамическому перемещению с учетом физики, сам определяя оптимальные действия для достижения цели.


Пример 2: Создать агента, который играет на стороне противника в PvP-игре, адаптируясь к стилю игрока.

Написание правил для такого поведения вручную потребует учета огромного количества сценариев. Агента можно обучить на основе симуляций, что позволит ему лучше адаптироваться.


Когда проще использовать ML-Agent вместо программной реализации?:

В играх с физикой (например, гонки или симуляторы), где динамика движения объектов зависит от множества факторов, легче обучить агента, чем писать код для всех возможных ситуаций;

Если объект должен адаптироваться к изменению среды (например, случайно генерируемые уровни), ML-агент сможет быстро обучиться эффективным действиям в разных условиях;

Например, если требуется создать агента, который играет на музыкальных инструментах или создает уникальные паттерны для игры;

## Выводы

Знакомство с программными средствами для создания системы машинного обучения и ее интеграции в Unity. Сделаны выводы насчёт “коэффициент корреляции ” в скриптах и выводы о том, как он влияет на обучение модели. Изучено, какие параметры yaml-агента и как влияют на обучение модели. Приведены примеры, для каких игровых задачи и ситуаций могут использоваться примеры 1 и 2 с ML-Agent’ом, а также в каких случаях проще использовать ML-агент, а не писать программную реализацию решения.


## Powered by

**BigDigital Team: Denisov | Fadeev | Panov**
